{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Activation_function.ipynb","provenance":[],"authorship_tag":"ABX9TyM44Aqk5NikoGx5A9xiGTwt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Проверим утверждение про затухание градиента на практике. В документации pytorch можно найти следующие функции активации (самые популярные мы подсветили жирным шрифтом.): \n","\n","ELU, Hardtanh, LeakyReLU, LogSigmoid, PReLU, ReLU, ReLU6, RReLU, SELU, CELU, Sigmoid, Softplus, Softshrink, Softsign, Tanh, Tanhshrink, Hardshrink.\n","\n","Вам предстоит найти активацию, которая приводит к наименьшему затуханию градиента. \n","\n","Для проверки мы сконструируем SimpleNet, которая будет иметь внутри 3 fc-слоя, по 1 нейрону в каждом без bias'ов. Веса этих нейронов мы проинициализируем единицами. На вход в эту сеть будем подавать числа из нормального распределения. Сделаем 200 запусков (NUMBER_OF_EXPERIMENTS) для честного сравнения и посчитаем среднее значение градиента в первом слое. Найдите такую функцию, которая будет давать максимальные значения градиента в первом слое. Все функции активации нужно инициализировать с аргументами по умолчанию (пустыми скобками)."],"metadata":{"id":"cB_2K1do8t3t"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"KXpO0O2l8pTM"},"outputs":[],"source":["import torch\n","import numpy as np\n","from torch.nn import *\n","\n","seed = int(input())\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","\n","NUMBER_OF_EXPERIMENTS = 200\n","\n","class SimpleNet(torch.nn.Module):\n","    def __init__(self, activation):\n","        super().__init__()\n","\n","        self.activation = activation\n","        self.fc1 = torch.nn.Linear(1, 1, bias=False)  # one neuron without bias\n","        self.fc1.weight.data.fill_(1.)  # init weight with 1\n","        self.fc2 = torch.nn.Linear(1, 1, bias=False)\n","        self.fc2.weight.data.fill_(1.)\n","        self.fc3 = torch.nn.Linear(1, 1, bias=False)\n","        self.fc3.weight.data.fill_(1.)\n","\n","    def forward(self, x):\n","        x = self.activation(self.fc1(x))\n","        x = self.activation(self.fc2(x))\n","        x = self.activation(self.fc3(x))\n","        return x\n","\n","    def get_fc1_grad_abs_value(self):\n","        return torch.abs(self.fc1.weight.grad)\n","\n","def get_fc1_grad_abs_value(net, x):\n","    output = net.forward(x)\n","    output.backward()  # no loss function. Pretending that we want to minimize output\n","                       # In our case output is scalar, so we can calculate backward\n","    fc1_grad = net.get_fc1_grad_abs_value().item()\n","    net.zero_grad()\n","    return fc1_grad\n","\n","activs = [ELU, Hardtanh, LeakyReLU, LogSigmoid, PReLU, ReLU, ReLU6, RReLU, SELU, CELU, Sigmoid, Softplus, Softshrink, Softsign, Tanh, Tanhshrink, Hardshrink]\n","\n","for a in activs:\n","   activation =  a()\n","   \n","net = SimpleNet(activation=activation)\n","fc1_grads = []\n","for x in torch.randn((NUMBER_OF_EXPERIMENTS, 1)):\n","    fc1_grads.append(get_fc1_grad_abs_value(net, x))\n","\n","# Проверка осуществляется автоматически, вызовом функции:\n","# print(np.mean(fc1_grads))\n","# (раскомментируйте, если решаете задачу локально)"]}]}